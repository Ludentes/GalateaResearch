# Galatea Development Roadmap

> **Vision:** Create off-the-shelf developer agents by shadowing real professionals, learning their unique processes, and deploying agents that behave like trained team members.

**Architecture Principle:** Build **two things** (Homeostasis + Memory-with-lifecycle). Everything else leverages the ecosystem (LLM, Skills, MCP, Agent Teams).

---

## Phase Overview

| Phase | Status | Focus | Key Deliverables | Duration |
|-------|--------|-------|------------------|----------|
| **Phase A** | âœ… Complete | Foundation | Chat UI, multi-provider streaming, PostgreSQL setup | 2 weeks |
| **Phase B** | âœ… Complete | Shadow Learning | Transcript extraction, knowledge store, context assembly | 1 week |
| **Phase C** | âœ… Complete | Observation + Homeostasis | OTEL pipeline, L0-L2 thinking, auto-extraction hooks | 1 week |
| **Phase D** | âœ… Complete | Formalize + Close the Loop | BDD integration tests, entity retrieval, tick(), supersession, config system, pipeline trace | 2 weeks |
| **Phase E** | ðŸ’­ Concept | Homeostasis Refinement | L2 LLM assessment, L3 meta-assessment, memory lifecycle, self-model | 1 week |
| **Phase F** | ðŸ’­ Concept | Skills + Visualization | SKILL.md auto-generation, heartbeat loop, dashboard, safety | 2 weeks |

**Total estimated time:** 8-9 weeks (~2 months)

**Phase D restructure (2026-02-13):** End-to-end trace revealed the feedback loop is broken â€” knowledge extracted but never used (`retrievedFacts: []`). Phase D reprioritized from homeostasis refinement to closing the loop. Old Phase D content (L2/L3, decay, consolidation) moved to Phase E. See `docs/plans/2026-02-13-phase-d-revised.md`.

---

## Phase A: Foundation âœ…

**Goal:** Establish core infrastructure for a functional chat agent.

### Key Deliverables
1. âœ… **TanStack Start** full-stack framework with SSR
2. âœ… **Chat UI** with message streaming and session management
3. âœ… **Multi-provider LLM support** (OpenAI, Claude, Ollama) via AI SDK v6
4. âœ… **PostgreSQL** database with Drizzle ORM
5. âœ… **Session management** (create, list, load, delete)
6. âœ… **Basic memory types** (stubs for future expansion)

### Architecture Decisions
- **UI Framework:** TanStack Start (React + TanStack Router + Server Functions)
- **LLM Abstraction:** AI SDK v6 (Vercel) for streaming and multi-provider support
- **Database:** PostgreSQL 16 with Drizzle ORM (type-safe SQL)
- **Deployment:** Docker Compose for local development

### Testing
- Unit tests for provider factories and chat logic
- Integration test with real LLM (Ollama)
- Manual testing: send messages, see responses

### Reference
- Plan: `docs/plans/2026-02-04-phase1-foundation.md`
- Progress: `docs/plans/2026-02-04-phase1-progress.md`

---

## Phase B: Shadow Learning Pipeline âœ…

**Goal:** Automate knowledge extraction from Claude Code session transcripts and wire learned knowledge into the Galatea chat agent's context.

### Key Deliverables
1. âœ… **Transcript Reader** â€” Parse Claude Code JSONL session files
2. âœ… **Signal Classifier** â€” Filter noise using regex patterns (from v1 gatekeeper)
3. âœ… **Knowledge Extractor** â€” LLM extraction via AI SDK `generateObject` + Zod
4. âœ… **Knowledge Store** â€” JSONL-based storage with deduplication
5. âœ… **Context Assembler** â€” Read knowledge + preprompts â†’ system prompt
6. âœ… **Extraction Pipeline** â€” Orchestrates read â†’ classify â†’ extract â†’ store
7. âœ… **API Endpoint** â€” `/api/extract` to trigger extraction on demand
8. âœ… **CLI Tool** â€” `pnpm extract <session-id>` for manual testing

### Architecture
Six-module pipeline: Transcript Reader â†’ Signal Classifier â†’ Knowledge Extractor â†’ Knowledge Store (JSONL) â†’ Context Assembler â†’ Chat Integration

### Quality Metrics
- **Precision:** 95% (validated in shadow learning experiment)
- **Recall:** 87%
- **Deduplication:** 3-path strategy (exact match, semantic similarity, source-level guard)

### Testing
- Unit tests for each module with fixtures
- Integration test for full pipeline (sample-session.jsonl â†’ knowledge.jsonl)
- Quality tests: precision/recall on reference transcripts
- Manual test: Extract from real session, verify knowledge appears in chat

### Reference
- Plan: `docs/plans/2026-02-11-phase-b-shadow-learning.md`
- Experiment: `docs/plans/2026-02-11-shadow-learning-experiment.md`
- Learning Scenarios: `docs/plans/2026-02-11-learning-scenarios.md`

---

## Phase C: Observation + Homeostasis Integration âœ…

**Goal:** Build OTEL observation infrastructure, implement homeostasis sensor with L0-L2 multi-level thinking, and enable automatic extraction via Claude Code hooks.

### Key Deliverables
1. âœ… **Extraction State Tracking** â€” Prevent re-processing of sessions
2. âœ… **SessionEnd Auto-Extraction Hook** â€” Trigger extraction when sessions end
3. âœ… **Homeostasis Sensor Module** â€” Assess 6 psychological dimensions
4. âœ… **L0-L2 Multi-Level Thinking** â€” Caching, heuristics, LLM placeholders
5. âœ… **Context Assembler Integration** â€” Inject homeostasis guidance into prompts
6. âœ… **OTEL Collector Docker Setup** â€” Receive and route observation events
7. âœ… **Observation Ingest API** â€” `/api/observation/ingest` for OTLP logs
8. âœ… **Event Store** â€” JSONL-based storage for observation events
9. âœ… **Claude Code OTEL Hooks** â€” Real-time event emission (UserPromptSubmit, PostToolUse)

### Architecture: L0-L2 Multi-Level Thinking

| Level | Description | Latency | Use Cases |
|-------|-------------|---------|-----------|
| **L0** | Cached/reflexive assessment | 0ms | Recent assessments within TTL |
| **L1** | Computed heuristics with relevance scoring | 1-5ms | knowledge_sufficiency, progress_momentum, communication_health, productive_engagement |
| **L2** | LLM semantic understanding (placeholder) | 2-5s | certainty_alignment, knowledge_application |

### Homeostasis Dimensions

| Dimension | L0 Cache TTL | L1/L2 | Triggers When... |
|-----------|--------------|-------|------------------|
| `knowledge_sufficiency` | 0ms | L1 | No relevant facts for user question |
| `progress_momentum` | 2 min | L1 | User repeating similar questions (stuck) |
| `communication_health` | 30 min | L1 | Session stale (4+ hours) |
| `productive_engagement` | 0ms | L1 | No task, empty conversation |
| `certainty_alignment` | 1 min | L2 | Agent confidence mismatch (Phase D) |
| `knowledge_application` | 5 min | L2 | Agent ignoring available facts (Phase D) |

### Evaluation Results

**Baseline (simple counting) vs L0-L2 (relevance scoring + caching):**

| Metric | Baseline | L0-L2 | Improvement |
|--------|----------|-------|-------------|
| **Failed Tests** | 6 | 4 | âœ… **-33%** |
| **Passing Tests** | 9 | 11 | âœ… **+22%** |

**Key win:** L1 relevance scoring successfully filters irrelevant facts (S1.3 test).

**Known edge cases (4 todo tests for Phase D):**
- 2Ã— stuck detection Jaccard similarity bug
- 1Ã— keyword matching strictness ("auth" vs "authentication")
- 1Ã— cascading goal achievement test

### Testing
- 17 automated evaluation tests based on learning scenarios
- Unit tests for extraction state, homeostasis engine, context assembler
- Integration tests for OTEL pipeline
- Manual test guide (964 lines) covering all 7 tasks with step-by-step verification

### Reference
- Evaluation Report: `docs/plans/2026-02-12-homeostasis-l0-l2-evaluation-report.md`
- Manual Test Guide: `docs/plans/2026-02-12-phase-c-manual-testing-guide.md`
- Commits: 11 commits on `feat/phase-c` branch

---

## Phase D: Formalize + Close the Loop âœ…

**Goal:** Turn the end-to-end trace into executable BDD integration tests, then close the feedback loop so extracted knowledge flows into agent behavior.

**Motivation:** End-to-end trace (2026-02-13) revealed the feedback loop is broken. Knowledge gets extracted but `retrievedFacts` is always `[]`. Closing the loop is higher priority than homeostasis refinement.

### Key Deliverables

**D.1: Formalize (Red)**
1. âœ… **Scenario Builder** â€” TestWorld helper with fixture seeding (real DB + real Ollama)
2. âœ… **Layer 1 Integration Tests** â€” Developer chat path (6 green, 4 todo)
3. âœ… **Layer 2 Integration Tests** â€” Extraction pipeline (5 green, 4 todo)
4. âœ… **Layer 3 Integration Tests** â€” tick() decisions (7 green, 2 todo)
5. âœ… **Mermaid Diagrams** â€” Sequence diagrams as visual sanity check

**D.2: Close the Loop (Green)**
6. âœ… **Entity-Based Fact Retrieval** â€” Hybrid retrieval: entity mentions + keyword overlap
7. âœ… **Wire Retrieval into Chat** â€” Both `sendMessageLogic` and `streamMessageLogic`
8. âœ… **tick() Function + Agent State** â€” 4-stage pipeline: self-model â†’ homeostasis â†’ channels â†’ action
9. âœ… **Supersession Logic** â€” `supersedeEntry()` + filtered from all retrieval paths
10. âœ… **Clean Up Dead Artifacts** â€” Removed knowledge.md rendering from pipeline

**D.3: Pipeline Debugging Infrastructure (unplanned, added mid-phase)**
11. âœ… **Config YAML** â€” Single source of truth for ~40 thresholds (`server/engine/config.yaml`)
12. âœ… **Pipeline Trace** â€” Opt-in per-entry decision log in fact retrieval
13. âœ… **Trace CLI** â€” `pnpm exec tsx scripts/trace.ts "query"` with auto-diagnosis
14. âœ… **Verification Scripts** â€” 11 standalone scripts in `scripts/verify/`

### Results

- 163 tests passing (25 test files)
- 18/28 integration tests green, 10 todo (6 for Phase E, 4 ready to flip)
- Feedback loop closed: extract â†’ store â†’ retrieve â†’ use â†’ assess
- All ~40 magic numbers consolidated into documented config.yaml

### Bug Fixes During Verification
- Entity matching: added content-text search (real data lacks structured `about` fields)
- ESM `__dirname`: fixed for Vite/Nitro runtime (`import.meta.url`)
- Keyword retrieval: added stop word filtering, lowered overlap threshold
- Port mismatch: Galatea runs on 13000, not 3000

### Reference
- Revised Plan: `docs/plans/2026-02-13-phase-d-revised.md`
- Manual Verification: `docs/verification/2026-02-15-phase-d-manual-verification.md`
- End-to-End Trace: `docs/plans/2026-02-13-end-to-end-trace.md`

---

## Phase E: Homeostasis Refinement + Memory Lifecycle ðŸ’­

**Goal:** Make homeostasis smarter (L2/L3), add memory lifecycle (decay, consolidation), implement self-model for powered-down mode, and add app-level OTEL events. Content moved from old Phase D + new items from end-to-end trace.

### Planned Deliverables
1. ðŸ’­ **Fix L1 Edge Cases** â€” Keyword stemming + stuck detection debugging
2. ðŸ’­ **L2: certainty_alignment** â€” LLM assessment for confidence mismatch
3. ðŸ’­ **L2: knowledge_application** â€” LLM assessment for knowledge usage
4. ðŸ’­ **L3 Meta-Assessment** â€” Arbitrate when L1 and L2 disagree
5. ðŸ’­ **Memory Consolidation** â€” Extract high-confidence patterns to CLAUDE.md
6. ðŸ’­ **Memory Decay** â€” Confidence reduction over time, archival below threshold
7. ðŸ’­ **Self-Model + Powered-Down Mode** â€” Resource/capacity/constraint awareness without LLM (X6)
8. ðŸ’­ **App-Level OTEL Events** â€” Chat, extraction, tick events (X3)
9. ðŸ’­ **Performance Monitoring** â€” L0 cache hit rate, L1/L2 latency tracking

### Success Criteria
- Remaining integration test todos from Phase D become green
- L2 latency < 3s, L3 disagreement < 10%
- Memory decay running, stale entries archived
- Self-model produces template responses in powered-down mode

### Reference
- Previous Phase D plan (now Phase E content): `docs/archive/completed/superseded/2026-02-12-phase-d-homeostasis-refinement.md`
- End-to-End Trace X6 (Self-Model): `docs/plans/2026-02-13-end-to-end-trace.md`

---

## Phase F: Skills + Visualization ðŸ’­

**Goal:** Auto-generate SKILL.md from patterns, enable heartbeat loop, build visualization dashboard, implement safety system.

### Planned Deliverables
1. ðŸ’­ **Pattern Detection** â€” Identify 3+ occurrences of similar procedures
2. ðŸ’­ **SKILL.md Auto-Generation** â€” Convert patterns to executable skills
3. ðŸ’­ **Heartbeat Loop** â€” `setInterval(() => tick("heartbeat"), 30_000)` (tick exists from Phase D)
4. ðŸ’­ **L4 Strategic Analysis** â€” Cross-session pattern analysis
5. ðŸ’­ **Homeostasis Dashboard** â€” Real-time dimension visualization
6. ðŸ’­ **Memory Browser** â€” Explore knowledge store, search, edit entries
7. ðŸ’­ **Safety & Boundaries** â€” Knowledge store poisoning guard, pre/post filters (X2)
8. ðŸ’­ **Contradiction Resolution** â€” Handle conflicting knowledge (advanced supersession)

### Success Criteria
- 3+ skills auto-generated from real usage patterns
- Heartbeat loop enables idle agent behaviors
- Dashboard shows real-time dimension state
- All integration test todos from Phase D/E become green

### Estimated Timeline
**2 weeks**

---

## Deferred / Out of Scope

### Memory Tier 3 (RAG/Mem0)
**Status:** Deferred until CLAUDE.md proves insufficient

**Trigger conditions:**
- CLAUDE.md exceeds 50KB (too large for every request)
- Agent needs cross-session pattern analysis beyond L4
- Multi-agent coordination requires shared memory

**Options:**
- Mem0 (managed semantic memory)
- Graphiti + FalkorDB (graph-based)
- Custom vector store (Pinecone, Weaviate)

### Multi-Agent Coordination
**Status:** Deferred to post-MVP

**Requirements:**
- Agent registry (who's available, what skills they have)
- Shared memory tier (CLAUDE.md/SKILL.md namespace)
- Agent-to-agent messaging (MCP server?)

**Use case:** Assign tasks to specialist agents (e.g., "Ask the DevOps agent")

### HomeAssistant + Frigate Integration
**Status:** Deferred to post-MVP

**Requirements:**
- MQTT-to-OTEL bridge (convert HA events to OTEL format)
- Camera stream analysis (Frigate events â†’ knowledge extraction)
- Privacy controls (PII filtering)

**Use case:** "The doorbell rang 3 times today" â†’ agent learns household patterns

---

## Key Architectural Decisions

### 1. Why L0-L2 Before L3-L4?
**Reason:** Establish fast path (L0/L1) and validate improvement (33% fewer failures) before adding complexity (L3/L4).

### 2. Why JSONL Over PostgreSQL for Knowledge?
**Reason:** Simplicity for Tier 1 memory. PostgreSQL overhead not justified until Tier 3 (RAG).

### 3. Why Ollama `glm-4.7-flash` for L2?
**Reason:** Fast local inference (2-5s), free, private. Claude API too expensive for frequent assessments.

### 4. Why Skills Over Custom Code?
**Reason:** Skills (SKILL.md) are:
- Portable (copy to any Claude Code session)
- User-editable (plain markdown)
- Ecosystem-aligned (Claude's native format)

### 5. Why OTEL Over Custom Events?
**Reason:** Industry standard, rich tooling (Collector, Jaeger, Prometheus), vendor-neutral.

---

## Success Metrics

### Phase B (Shadow Learning)
- âœ… Precision: 95%
- âœ… Recall: 87%
- âœ… Deduplication: 3-path strategy prevents duplicates

### Phase C (Homeostasis L0-L2)
- âœ… Test improvement: 33% fewer failures, 22% more passing
- âœ… L1 relevance scoring: Filters irrelevant facts correctly

### Phase D (Formalize + Close the Loop)
- âœ… 163 tests passing (25 test files)
- âœ… Feedback loop: extract â†’ retrieve â†’ use in context
- âœ… tick() 4-stage pipeline with API endpoint
- âœ… Config system: ~40 thresholds in documented YAML
- âœ… Pipeline trace for debugging retrieval issues

### Phase E (Homeostasis Refinement + Memory Lifecycle)
- ðŸŽ¯ Remaining integration test todos become green
- ðŸŽ¯ L2 latency < 3s, L3 disagreement < 10%
- ðŸŽ¯ Memory decay running, stale entries archived
- ðŸŽ¯ Self-model produces template responses in powered-down mode

### Phase F (Skills + Visualization)
- ðŸŽ¯ 3+ skills auto-generated from real patterns
- ðŸŽ¯ L4 provides cross-session insights
- ðŸŽ¯ All 9 learning scenarios pass end-to-end

---

## Risk Management

### Technical Risks

| Risk | Likelihood | Impact | Mitigation |
|------|------------|--------|------------|
| L2 LLM calls too slow | Medium | High | Use fast local model, 60s cache, async calls |
| CLAUDE.md grows too large | Medium | Medium | Implement Tier 3 (RAG) if exceeds 50KB |
| Skill quality varies | High | Medium | Human review loop, skill validation tests |
| Memory decay too aggressive | Low | Low | Tune decay formula (0.95 â†’ 0.97?) |

### Product Risks

| Risk | Likelihood | Impact | Mitigation |
|------|------------|--------|------------|
| Users don't trust auto-extracted knowledge | Medium | High | Show sources, confidence scores, allow editing |
| Skills don't generalize across users | High | Medium | Focus on project-specific patterns, not universal truths |
| Homeostasis guidance too frequent | Low | Low | Tune dimension thresholds, add cooldown periods |

---

## Dependencies

### External
- **Claude Code** â€” Session transcripts (JSONL format)
- **Ollama** â€” Local LLM for extraction and L2 assessment
- **Docker** â€” PostgreSQL, OTEL Collector
- **AI SDK v6** â€” LLM abstraction layer

### Internal (between phases)
- Phase C â†’ Phase D: L0-L2 architecture, evaluation tests
- Phase D â†’ Phase E: L2/L3 assessment, memory consolidation
- Phase E â†’ Phase F: SKILL.md auto-generation, L4 analysis

---

## Team & Timeline

**Current velocity:** ~1 week per phase (Phases A-C completed in 4 weeks)

**Projected completion:**
- Phase D: Week 5
- Phase E: Weeks 6-7
- Phase F: Week 8

**Total:** 8 weeks (2 months) to production-ready v1

---

## Learning Scenarios Coverage

| Scenario | Phase B | Phase C | Phase D | Phase E | Phase F |
|----------|---------|---------|---------|---------|---------|
| L1: No knowledge (extract + retrieve) | âœ… | âœ… | âœ… | âœ… | âœ… |
| L2: Conflicting knowledge (dedup) | âœ… | âœ… | âœ… | âœ… | âœ… |
| L3: Pattern recognition (3+ occurrences) | â¸ï¸ | â¸ï¸ | â¸ï¸ | â¸ï¸ | ðŸŽ¯ |
| L4: Knowledge application (use in response) | âœ… | âœ… | âœ… | âœ… | âœ… |
| L5: Uncertainty handling (confidence) | â¸ï¸ | â¸ï¸ | âœ… | âœ… | âœ… |
| L6: Knowledge staleness (decay) | â¸ï¸ | â¸ï¸ | â¸ï¸ | ðŸŽ¯ | âœ… |
| L7: Idle agent (heartbeat) | â¸ï¸ | â¸ï¸ | â¸ï¸ | â¸ï¸ | ðŸŽ¯ |
| L8: Cross-session patterns (L4) | â¸ï¸ | â¸ï¸ | â¸ï¸ | â¸ï¸ | ðŸŽ¯ |
| L9: Proactive suggestions (SKILL.md) | â¸ï¸ | â¸ï¸ | â¸ï¸ | â¸ï¸ | ðŸŽ¯ |

**Legend:**
- âœ… Fully implemented
- ðŸŽ¯ Planned in this phase
- â¸ï¸ Deferred to later phase

---

## Recommendations

### For Phase E
1. **Fix L1 edge cases first** â€” Low-hanging fruit, unblocks evaluation tests (4 todos)
2. **Prototype L2 with mocked LLM** â€” Validate prompt design before implementing real calls
3. **Memory decay formula** â€” Start conservative (0.95^days), tune with real data
4. **Flip 4 integration test todos** â€” Already passing, just need `it.todo()` â†’ `it()`

### For Phase F
1. **Start simple with SKILL.md** â€” Basic pattern detection before advanced cross-session analysis
2. **Heartbeat cost analysis** â€” Periodic LLM calls add up, consider free local models only
3. **Dogfood the dashboard** â€” Use it internally first, iterate on UX

---

*Last Updated: 2026-02-15*
*Current Phase: Phase D complete, Phase E planned*
